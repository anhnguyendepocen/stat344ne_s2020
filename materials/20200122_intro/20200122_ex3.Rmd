---
title: "Motivating Example 3"
subtitle: "Jan. 22, 2020"
output:
  pdf_document:
    keep_tex: true
    highlight: zenburn
header-includes:
   - \usepackage{soul}
   - \usepackage{booktabs}
documentclass: extarticle
geometry: margin=0.6in
---

```{r global_options, include=FALSE}
library(knitr)
library(reticulate)
use_python("/Users/eray/anaconda3/bin/python")
knitr::opts_chunk$set(eval = TRUE, echo=FALSE, engine='python')
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## Example

Here's a fake data set with two quantitative features (also known as explanatory variables, independent variables, or inputs) and one categorical response (dependent variable, output):

```{python}
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
```

```{python, echo = FALSE, }
# adapted from http://cs231n.github.io/neural-networks-case-study/
# set seed for reproducibility
np.random.seed(3)

# sample size per group
n = 100

k = 4 # number of branches
x = np.zeros((2, n*k)) # data matrix (each row = single example)
y = np.zeros((1, n*k)) # class labels
for j in range(k):
  ix = range(n*j,n*(j+1))
  r = np.linspace(0.0,1,n) # radius
  t = np.linspace(j * 2 * 2 * np.pi / k + j * 2 * np.pi / k, (j+1) * 2 * 2 * np.pi / k + (j+1) * 2 * np.pi / k,n) + np.random.randn(n)*0.2 # theta
  x[:, ix] = np.c_[r*np.sin(t), r*np.cos(t)].T
  y[:, ix] = np.mod(j, 2)

# make a plot
fig, ax = plt.subplots()
scatter = ax.scatter(x[0, ], x[1, ], c = y[0, ], norm = plt.Normalize(-0.2, 1.2), cmap = plt.get_cmap('plasma'))
plt.show()
```

The example is adapted from one at http://cs231n.github.io/neural-networks-case-study/

#### (a) On the plot, draw your guess at a good decision boundary: you will predict that y = 0 (plotted in blue) on one side of the decision boundary and y = 1 (plotted in orange) on the other side of the decision boundary.

#### (b) What model would you use for these data?







